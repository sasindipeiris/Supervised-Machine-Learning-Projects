# Birdclef
This notebook presents a complete machine learning pipeline designed to classify species from acoustic recordings, contributing to biodiversity monitoring through Passive Acoustic Monitoring (PAM). The context of the task is ecologically significant: many mobile and habitat-diverse species act as indicators of ecosystem health, and tracking their presence and activity through sound offers a scalable, non-invasive alternative to traditional observer-based surveys. The challenge lies in identifying bird species based on their vocalizations, often using limited labeled data collected from remote regions such as the Magdalena Valley in Colombia.

# Imports
To build this system, the notebook begins with importing a robust set of libraries spanning several domains. For general data handling and numerical computation, libraries like numpy, pandas, and json are used. Audio processing relies heavily on librosa, torchaudio, and soundfile—each of which supports different aspects such as loading waveforms, applying filters, and saving or manipulating audio formats. For visualization, tools like matplotlib, seaborn, and librosa.display enable spectral analysis and plotting, facilitating inspection of audio features. The deep learning core of the notebook is implemented using PyTorch, supported by utility modules like torch.nn, torch.utils.data, and the timm library for accessing pretrained computer vision models.

# Mel Spectrogram
A significant portion of the notebook is dedicated to explaining and configuring the use of Mel-spectrograms as the primary input feature for classification. The Mel-spectrogram is a time-frequency representation of an audio signal that transforms waveforms into a 2D image where time is plotted on the x-axis, frequency (scaled using the Mel scale) is on the y-axis, and color intensity indicates amplitude. The Mel scale is particularly well-suited for ecological audio because it mimics the nonlinear frequency perception of the human ear—denser at lower frequencies and sparser at higher frequencies. By converting audio into Mel-spectrograms, the model leverages a format that is both compact and perceptually relevant, allowing convolutional neural networks (CNNs), originally developed for image recognition, to be effectively reused in the audio domain.

# Configuration
To ensure reproducibility and consistent experimental results, a configuration class (CFG) is created to centralize hyperparameters, paths, and model settings. It includes specifications for sampling rate, audio duration, hop length between spectrogram frames, number of Mel frequency bands, and training hyperparameters like batch size and learning rate. A set_seed() function is defined to fix random seeds across random, numpy, and torch, which helps produce stable training curves and validation metrics during multiple runs.

# Data Loading and Preprocessing
Next, the notebook loads and processes the metadata associated with the audio clips. CSV files such as taxonomy.csv contain species labels, while train_metadata.csv includes file names, site locations, and timing. Often, these files include string representations of lists (e.g., secondary labels), which are converted into true Python lists using ast.literal_eval(). This preprocessing step is critical to enable proper label handling, especially when species co-occur in recordings. The data is then filtered, cleaned, and inspected visually to ensure consistency and readiness for feature extraction.

The audio clips are uniformly resampled and padded to a fixed duration, then transformed into Mel-spectrograms using librosa.feature.melspectrogram. The spectrograms are then log-scaled using decibel conversion to compress the dynamic range, making subtle frequency patterns more discernible to the model. This spectral data is treated as a grayscale image, enabling its use as input to CNN architectures. All preprocessing is encapsulated in a custom PyTorch Dataset class, which dynamically loads audio files, applies transformations, and returns spectrogram tensors along with their corresponding labels.

# Model architecture and Transfer Learning
The classification model is constructed using a pretrained CNN backbone from the timm library—typically architectures like ResNet or EfficientNet trained on ImageNet. These models are adapted by replacing their final fully connected layer with a new classifier head matching the number of bird species. The use of pretrained models is particularly advantageous in this task because labeled audio data is sparse and expensive to obtain, while the visual features extracted from spectrograms often share structural similarities with natural images, making transfer learning effective. The notebook resizes spectrogram inputs as necessary to conform to the expected input shape of these CNNs

# Training Pipeline
The training loop implements a standard supervised learning pipeline. Each batch is passed through the model to generate predictions, which are compared to the ground truth labels using the cross-entropy loss. The gradients are then backpropagated, and weights are updated using the Adam optimizer. After each epoch, the model is evaluated on a validation set to monitor performance. During validation, metrics such as accuracy, precision, recall, and F1-score are computed using functions from sklearn.metrics. These metrics are crucial in this context because ecological datasets often exhibit severe class imbalance—many species occur infrequently, and a high accuracy score might be misleading if the model simply favors majority classes. The F1-score, in particular, balances precision and recall, making it more suitable for biodiversity detection tasks where both false positives and false negatives can carry high ecological costs.

# Evaluation
Finally, the model is used to make predictions on the test set. These predictions are generated by passing audio files through the same preprocessing pipeline and model, then selecting the class with the highest softmax probability. The results are written to a CSV file in the format required for submission to the competition platform. This step completes the pipeline, translating complex field recordings into structured outputs that can inform biodiversity assessments.

# Summary
In summary, the notebook presents a methodologically sound and technically complete approach to ecological audio classification. It effectively bridges modern deep learning tools with signal processing techniques, using a data-efficient strategy that is particularly well-suited to conservation challenges. The pipeline is modular, allowing future enhancements such as semi-supervised learning, domain adaptation, or attention-based models to be integrated with minimal effort. Overall, this implementation not only meets the requirements of the BirdCLEF competition but also exemplifies best practices for acoustic species monitoring in real-world biodiversity research.
