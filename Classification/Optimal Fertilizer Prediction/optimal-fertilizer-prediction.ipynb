{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"sourceType":"competition"},{"sourceId":11592231,"sourceType":"datasetVersion","datasetId":7269189}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom IPython.display import display, Image\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:09:36.490155Z","iopub.execute_input":"2025-07-14T11:09:36.490411Z","iopub.status.idle":"2025-07-14T11:09:38.092855Z","shell.execute_reply.started":"2025-07-14T11:09:36.490392Z","shell.execute_reply":"2025-07-14T11:09:38.092102Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# II. Load Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")\noriginal = pd.read_csv(\"/kaggle/input/fertilizer-prediction/Fertilizer Prediction.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:09:41.232302Z","iopub.execute_input":"2025-07-14T11:09:41.232683Z","iopub.status.idle":"2025-07-14T11:09:42.747370Z","shell.execute_reply.started":"2025-07-14T11:09:41.232662Z","shell.execute_reply":"2025-07-14T11:09:42.746609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:09:43.323878Z","iopub.execute_input":"2025-07-14T11:09:43.324474Z","iopub.status.idle":"2025-07-14T11:09:43.348478Z","shell.execute_reply.started":"2025-07-14T11:09:43.324443Z","shell.execute_reply":"2025-07-14T11:09:43.347618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:09:46.472303Z","iopub.execute_input":"2025-07-14T11:09:46.472539Z","iopub.status.idle":"2025-07-14T11:09:46.579582Z","shell.execute_reply.started":"2025-07-14T11:09:46.472515Z","shell.execute_reply":"2025-07-14T11:09:46.578945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:09:49.960852Z","iopub.execute_input":"2025-07-14T11:09:49.961571Z","iopub.status.idle":"2025-07-14T11:09:49.993282Z","shell.execute_reply.started":"2025-07-14T11:09:49.961546Z","shell.execute_reply":"2025-07-14T11:09:49.992597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:09:53.752612Z","iopub.execute_input":"2025-07-14T11:09:53.752916Z","iopub.status.idle":"2025-07-14T11:09:53.775172Z","shell.execute_reply.started":"2025-07-14T11:09:53.752898Z","shell.execute_reply":"2025-07-14T11:09:53.774599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# III. Data Processing","metadata":{}},{"cell_type":"markdown","source":"## A. Isolate Target","metadata":{}},{"cell_type":"code","source":"# Drop the 'id' column\nX = train.drop(columns=['id', 'Fertilizer Name'])\n\n# Extract the target column\ny = train['Fertilizer Name']\n\nX.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:09:57.528572Z","iopub.execute_input":"2025-07-14T11:09:57.528857Z","iopub.status.idle":"2025-07-14T11:09:57.556539Z","shell.execute_reply.started":"2025-07-14T11:09:57.528840Z","shell.execute_reply":"2025-07-14T11:09:57.555947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# Output labels are now numbers\ny_encoded[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:10:01.727716Z","iopub.execute_input":"2025-07-14T11:10:01.727974Z","iopub.status.idle":"2025-07-14T11:10:01.854019Z","shell.execute_reply.started":"2025-07-14T11:10:01.727955Z","shell.execute_reply":"2025-07-14T11:10:01.853463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Just check\nX.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:10:04.809272Z","iopub.execute_input":"2025-07-14T11:10:04.809802Z","iopub.status.idle":"2025-07-14T11:10:04.885393Z","shell.execute_reply.started":"2025-07-14T11:10:04.809780Z","shell.execute_reply":"2025-07-14T11:10:04.884682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## B. Prepare Original Data","metadata":{}},{"cell_type":"code","source":"# Prepare multiple copies of original dataset\norig_copy = original.copy()\n\n# Number of copies\nn = 6\nfor i in range(n):\n    original = pd.concat([original, orig_copy], axis=0, ignore_index=True)\n    \noriginal.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:10:11.354497Z","iopub.execute_input":"2025-07-14T11:10:11.355173Z","iopub.status.idle":"2025-07-14T11:10:11.531397Z","shell.execute_reply.started":"2025-07-14T11:10:11.355152Z","shell.execute_reply":"2025-07-14T11:10:11.530805Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## C. Deal with Imbalance\n\nThe training data has significantly fewer DAP and Urea samples, which might explain the underperformance of these classes in past experiments. Adding multiple copies of the original data helps with this. Isolating relevant samples for an extra boost worked for the two classes, but it led to a drop in performance in the other classes. I'll update this notebook if I find a better way.","metadata":{}},{"cell_type":"code","source":"# Identify underrepresented classes\ntrain['Fertilizer Name'].value_counts(normalize=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:10:15.831780Z","iopub.execute_input":"2025-07-14T11:10:15.832246Z","iopub.status.idle":"2025-07-14T11:10:15.883603Z","shell.execute_reply.started":"2025-07-14T11:10:15.832226Z","shell.execute_reply":"2025-07-14T11:10:15.882869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify underrepresented classes\norig_copy['Fertilizer Name'].value_counts(normalize=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:10:19.401363Z","iopub.execute_input":"2025-07-14T11:10:19.401846Z","iopub.status.idle":"2025-07-14T11:10:19.413873Z","shell.execute_reply.started":"2025-07-14T11:10:19.401823Z","shell.execute_reply":"2025-07-14T11:10:19.413244Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IV. Modeling","metadata":{}},{"cell_type":"code","source":"# Store scores\nf1_scores = []\nmap3_scores = []\nmodels = []\n\n# Collect predictions and true labels across all folds\nall_y_true = []\nall_y_pred = []\n\n# Prepare K-Fold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded)):\n    print(f\"\\n***** Fold {fold + 1} *****\")\n\n    # Make full copies to avoid warnings\n    X_train = X.iloc[train_idx].copy()\n    X_val = X.iloc[val_idx].copy()\n    y_train = y_encoded[train_idx]\n    y_val = y_encoded[val_idx]\n\n    # Combine original with train data \n    X_train = pd.concat([X_train, original], ignore_index=True)\n    y_train = np.concatenate([y_train, le.transform(original['Fertilizer Name'])])\n\n    # Drop target column from training data\n    X_train.drop(columns=['Fertilizer Name'], inplace=True)\n\n    # Convert all features to categorical (except target, which is already separated)\n    for col in X_train.columns:\n        X_train[col] = X_train[col].astype('category')\n        \n    for col in X_val.columns:\n        X_val[col] = X_val[col].astype('category')\n    \n    cat_features = X_train.columns.tolist()   # capture all input columns\n\n    # For debugging purposes\n    # print(cat_features)\n    # print(X_train.info())\n    # print(X_val.info())\n\n    model = XGBClassifier(\n                max_depth=7,\n                colsample_bytree=0.4,\n                subsample=0.8,\n                n_estimators=20000,\n                learning_rate=0.01,\n                gamma=0.26,\n                max_delta_step=4,\n                reg_alpha=2.7,\n                reg_lambda=1.4,\n                objective='multi:softprob',\n                random_state=13,\n                enable_categorical=True,\n                tree_method='hist',     \n                device='cuda'  \n            )\n\n    model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_train, y_train),(X_val, y_val)],\n        early_stopping_rounds=100,\n        verbose=1000,\n    )\n    \n    # Predict class labels and probabilities\n    y_pred = model.predict(X_val)\n    y_probs = model.predict_proba(X_val)\n\n    # Store predictions and true labels\n    all_y_true.extend(y_val)\n    all_y_pred.extend(y_pred)\n\n    # F1 Score\n    report = classification_report(y_val, y_pred, output_dict=True)\n    f1_macro = report[\"macro avg\"][\"f1-score\"]\n    f1_scores.append(f1_macro)\n    \n    # MAP@3\n    top3_preds = np.argsort(y_probs, axis=1)[:, -3:][:, ::-1]\n    \n    def mapk(actual, predicted, k=3):\n        def apk(a, p, k):\n            if a in p[:k]:\n                return 1.0 / (p[:k].index(a) + 1)\n            return 0.0\n        return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n\n    map3 = mapk(y_val.tolist(), top3_preds.tolist(), k=3)\n    map3_scores.append(map3)\n    models.append(model)\n\n    print(f\"F1 (macro): {f1_macro:.4f} | MAP@3: {map3:.4f}\")\n\n# Final Results\nprint(\"\\n***** Final CV Results *****\")\nprint(f\"Avg F1: {np.mean(f1_scores):.4f}\")\nprint(f\"Avg MAP@3: {np.mean(map3_scores):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T11:10:22.885898Z","iopub.execute_input":"2025-07-14T11:10:22.886572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loss curves for the last fold\nresults = model.evals_result()\nplt.plot(results['validation_0']['mlogloss'], label='Train')\nplt.plot(results['validation_1']['mlogloss'], label='Val')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# V. Results Analysis","metadata":{}},{"cell_type":"markdown","source":"## A. Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# Step 1: Confusion matrix\ncm = confusion_matrix(all_y_true, all_y_pred)\ncm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n# Step 2: Make it pretty\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm_norm, annot=True, fmt=\".3f\", cmap=\"Greens\", \n            xticklabels=le.classes_, yticklabels=le.classes_,\n           )\n\nplt.title(\"Normalized Confusion Matrix\", fontsize=16)\nplt.xlabel(\"Predicted Label\", fontsize=12)\nplt.ylabel(\"True Label\", fontsize=12)\nplt.xticks(rotation=0)\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## B. Classification Report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_val, y_pred, digits=4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## C. Feature Importance\n\nSince fertilizers are mostly defined by their composition, it is logical to see that soil nutrients play a big role in product selection. They need to complement each other. \n\n* **CatBoost + Original Data:** Phosphorus is the most important factor, followed by nitrogen. Potasssium is much lower in the ranking, just behind moisture and crop type.\n* **CatBoost + 5x Original Data:** Phosphorus remains on top, now followed by Moisture, while Nitrogen slips to third. Potasssium rose to fourth.","metadata":{}},{"cell_type":"code","source":"# Initialize accumulator\nimportances_total = np.zeros(len(cat_features))\nfeature_names = cat_features\n\n# Accumulate importance per model\nfor model in models:\n    importances_total += model.feature_importances_\n\n# Average\nimportances_avg = importances_total / len(models)\n\n# Make a dataframe\nimportances_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importances_avg\n}).sort_values(by='importance', ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Limit to top N features for readability, default to all columns\ntop_n = len(X_train.columns)\ntop_features = importances_df.head(top_n)\n\n# Create a green color palette\ngreen_palette = sns.color_palette(\"Greens\", as_cmap=False, n_colors=len(top_features))\n\n# Plot using barplot\nplt.figure(figsize=(8, 5))\n\nsns.barplot(\n    data=top_features,\n    y='feature',\n    x='importance',\n    palette=green_palette\n)\n\nplt.title(\"Top Feature Importances\")\nplt.grid(axis='x', linestyle='--', alpha=0.6)\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## D. Top-N Coverage\n\nHow likely are we to get the right answer if we look at the top-N predictions? As you would expect, the chance goes higher the more we relax our standard. However, the competition limits us to the top 3.","metadata":{}},{"cell_type":"code","source":"def top_n_coverage(y_true, y_proba, n=3):\n    \"\"\"Returns the proportion of times the true label is in the top-N predicted labels.\"\"\"\n    top_n_preds = np.argsort(y_proba, axis=1)[:, -n:]  # Get top N indices (classes)\n    \n    # Check if the true label is in the top N predictions\n    hits = [y_true[i] in top_n_preds[i] for i in range(len(y_true))]\n    \n    return np.mean(hits)\n\nfor n in range(1, 8):\n    coverage = top_n_coverage(y_val, y_probs, n)\n    print(f\"Top-{n} Coverage: {coverage:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup: Top-N values\ntop_ns = list(range(1, y_probs.shape[1] + 1))\ncoverages = [top_n_coverage(y_val, y_probs, n) for n in top_ns]\n\n# Apply Seaborn theme\nsns.set(style=\"whitegrid\")\nsns.set_palette(\"Greens\")\n\n# Create figure\nplt.figure(figsize=(8, 5))\n\n# Plot with seaborn line aesthetics\nsns.lineplot(x=top_ns, y=coverages, marker='o', color='green', linewidth=2)\n\n# Decorations\nplt.title(\"Top-N Coverage Curve\", fontsize=16)\nplt.xlabel(\"N (Top-N Predictions)\", fontsize=12)\nplt.ylabel(\"Coverage\", fontsize=12)\nplt.ylim(0, 1.05)\nplt.xticks(top_ns)\nplt.yticks([i/10 for i in range(11)])\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.axvline(x=3, color='red', linestyle='--', linewidth=2, label='Top-3 Threshold')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = test.drop(columns='id')\n\n# Convert test data\nfor col in X_test.columns:\n    X_test[col] = X_test[col].astype('category')\n\n# Accumulate prediction probabilities\nall_preds = np.zeros((test.shape[0], len(le.classes_)))\n\nfor model in models:\n    probs = model.predict_proba(X_test)\n    all_preds += probs\n\n# Average over folds\navg_preds = all_preds / len(models)\n\n# Get top 3 indices like before\ntop3_preds = np.argsort(avg_preds, axis=1)[:, -3:][:, ::-1]  # Top 3 class indices, descending order\n\n# Convert class indices back to original label strings\ntop3_labels = le.inverse_transform(top3_preds.ravel()).reshape(top3_preds.shape)\n\nsubmission = pd.DataFrame({\n    'id': test['id'],  # Replace with actual ID column name\n    'Fertilizer Name': [' '.join(row) for row in top3_labels]\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Done!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VI. Submission","metadata":{}},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}